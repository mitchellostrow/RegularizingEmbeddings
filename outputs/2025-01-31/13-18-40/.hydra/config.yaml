training:
  wandb:
    project: sequence_model
    name: ${now:%Y-%m-%d_%H-%M-%S}
  training:
    max_epochs: 100
    patience: 50
  data:
    __target__: src.lightning.data.SequenceDataModule
    batch_size: 32
    num_workers: 4
    pin_memory: true
  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.001
    weight_decay: 0.0
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: min
    factor: 0.1
    patience: 5
  criterion:
    _target_: torch.nn.MSELoss
  paths:
    output_dir: checkpoints
model:
  _target_: RegularizingEmbeddings.models.lru.LRUMinimal
  input_dim: null
  d_model: 256
  d_state: 64
  mlp_hidden: 1024
  siso: false
  rmin: 0.8
  rmax: 0.99
  dropout: 0.1
data:
  flow:
    _target_: RegularizingEmbeddings.dysts_sim.flows.Lorenz
    random_state: 42
    dt: null
  trajectory_params:
    n_periods: 12
    method: Radau
    resample: true
    pts_per_period: 100
    return_times: true
    standardize: false
    noise: 0.0
    num_ics: 20
    new_ic_mode: reference
    traj_offset_sd: 0.01
    verbose: true
  postprocessing:
    obs_noise: 0
    dims_to_observe:
    - 0
    filter_data: false
    low_pass: 10
    high_pass: null
  train_test_params:
    seq_length: 25
    seq_spacing: 1
    train_percent: 0.7
    test_percent: 0.1
    split_by: trajectory
    dtype: torch.FloatTensor
    verbose: true
    delay_embedding_params:
      observed_indices: all
      n_delays: 1
      delay_spacing: 1
logger: wandb
